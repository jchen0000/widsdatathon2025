---
title: "Data exploration"
author: "LYU JING"
date: "2025-02-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Install and Load Necessary Libraries
if (!require(tidyverse)) install.packages("tidyverse")
if (!require(readxl)) install.packages("readxl")
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(caret)) install.packages("caret")
if (!require(data.table)) install.packages("data.table")
if (!require(corrplot)) install.packages("corrplot")
if (!require(Rtsne)) install.packages("Rtsne")  # For dimensionality reduction in functional connectome
if (!require(Matrix)) install.packages("Matrix")

library(tidyverse)
library(readxl)
library(ggplot2)
library(caret)
library(data.table)
library(corrplot)
#library(Rtsne)
library(Matrix)
library(mice)
library(Hmisc)
```

```{r}
# Load necessary libraries
library(readxl)  # For reading Excel files
library(readr)   # For reading CSV files
setwd("/Users/jinglv/Desktop/RA/widsdatathon2025")

path <- "data/train"  # Update this with your actual path
path2<-"/Users/jinglv/Downloads"
path3<-"data/test" 


# Read the Excel files
train_categorical_metadata <- read_excel(file.path(path, "TRAIN_CATEGORICAL_METADATA.xlsx"))
train_functional_connectome <- read_csv(file.path(path2, "TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv"))
train_quantitative_metadata <- read_excel(file.path(path, "TRAIN_QUANTITATIVE_METADATA.xlsx"))
training_solutions <- read_excel(file.path(path, "TRAINING_SOLUTIONS.xlsx"))

test_categorical_metadata <- read_excel(file.path(path3, "TEST_CATEGORICAL.xlsx"))
test_functional_connectome <- read_csv(file.path(path2, "TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv"))
test_quantitative_metadata <- read_excel(file.path(path3, "TEST_QUANTITATIVE_METADATA.xlsx"))



# Display the first few rows of each dataset
head(train_categorical_metadata)
head(train_functional_connectome)
head(train_quantitative_metadata)
head(training_solutions)


summary(train_functional_connectome$`110throw_153thcolumn`)
```


# missing
```{r}


# Check for Missing Values
missing_values <- function(df, name) {
  print(paste("Missing Values in:", name))
  print(colSums(is.na(df)))
}
train_categorical_metadata[train_categorical_metadata == "NA"] <- NA
train_functional_connectome[train_functional_connectome == "NA"] <- NA
train_quantitative_metadata[train_quantitative_metadata == "NA"] <- NA

test_categorical_metadata[test_categorical_metadata == "NA"] <- NA
test_functional_connectome[test_functional_connectome == "NA"] <- NA
test_quantitative_metadata[test_quantitative_metadata == "NA"] <- NA


training_solutions[training_solutions == "NA"] <- NA

train_quantitative_metadata$MRI_Track_Age_at_Scan = as.numeric(train_quantitative_metadata$MRI_Track_Age_at_Scan)

missing_values(train_categorical_metadata, "Categorical Metadata")
#missing_values(train_functional_connectome, "Functional Connectome")
missing_values(train_quantitative_metadata, "Quantitative Metadata")
missing_values(training_solutions, "Training Solutions")

missing_values(test_categorical_metadata, "Categorical Metadata")
#missing_values(train_functional_connectome, "Functional Connectome")
missing_values(test_quantitative_metadata, "Quantitative Metadata")

# XGBoost!!!
# KNN;imputation!!

# Function to Check for Missing Values in Any Dataset
missing_values2 <- function(df, name) {
  missing_data <- colSums(is.na(df))
  missing_data <- missing_data[missing_data > 0]  # Filter only columns with missing values
  
  if (length(missing_data) == 0) {
    print(paste("No missing values found in:", name))
  } else {
    print(paste("Missing Values in:", name))
    print(missing_data)
  }
}

# Check for Missing Values in Functional Connectome Dataset
missing_values2(train_functional_connectome, "Functional Connectome")

# ----- 1. Data Visualization -----
```



```{r}
# Load necessary libraries
library(VIM)
library(dplyr)

# Convert character columns to factors
train_categorical_metadata <- train_categorical_metadata %>%
  mutate(across(where(is.character), as.factor))

# Perform kNN imputation
# k = 5 is common; adjust as needed
# imp_var = FALSE avoids creating *_imp columns for missing indicators
imputed_data <- kNN(train_categorical_metadata, k = 5, imp_var = FALSE)

```
  
```{r}
# Update the original datasets with imputed values
train_categorical_metadata$PreInt_Demos_Fam_Child_Ethnicity <- imputed_data$PreInt_Demos_Fam_Child_Ethnicity

# Verify that missing values have been imputed
missing_values(train_categorical_metadata, "Categorical Metadata (Post-Imputation)")


library(writexl)

# Define the full file path
file_path <- file.path(path, "TRAIN_CATEGORICAL_METADATA_imputed_Jing.xlsx")

# Save the DataFrame to the specified Excel file
write_xlsx(train_categorical_metadata, file_path)
```


train and test combine together and then impute;



# age predict model
```{r}

library(glmnet)
library(dplyr)



# Merge datasets on 'participant_id'
p_data <- train_functional_connectome %>%
  inner_join(train_quantitative_metadata, by = "participant_id")



# Clean column names
colnames(p_data) <- make.names(colnames(p_data), unique = TRUE)
colnames(p_data) <- gsub("\\.", "_", colnames(p_data))

```

```{r}
# Subset with complete MRI_Track_Age_at_Scan
train_data <- p_data %>%
  filter(!is.na(MRI_Track_Age_at_Scan))

# Subset with missing MRI_Track_Age_at_Scan
impute_data <- p_data %>%
  filter(is.na(MRI_Track_Age_at_Scan))

# Define predictor variables (excluding 'participant_id' and 'MRI_Track_Age_at_Scan')
predictors <- setdiff(names(train_data), c("participant_id", "MRI_Track_Age_at_Scan"))

# Create predictor matrix (X) and response vector (y)
X_train <- as.matrix(train_data[, predictors])
y_train <- train_data$MRI_Track_Age_at_Scan

```

```{r}
# Perform cross-validated Lasso regression
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)

# Extract the best lambda value
best_lambda <- cv_lasso$lambda.min

```

```{r}
# Create predictor matrix for imputation
X_impute <- as.matrix(impute_data[, predictors])

# Predict missing values
imputed_values <- predict(cv_lasso, s = best_lambda, newx = X_impute)

# Impute the predicted values
p_data$MRI_Track_Age_at_Scan[is.na(p_data$MRI_Track_Age_at_Scan)] <- imputed_values

common_columns <- intersect(colnames(p_data), colnames(train_quantitative_metadata))

p_data_selected <- select(p_data, all_of(common_columns))
p_data_selected$MRI_Track_Age_at_Scan

# Define the full file path
file_path <- file.path(path, "TRAIN_QUANTITATIVE_METADATA_imputed_Jing.xlsx")

# Save the DataFrame to the specified Excel file
write_xlsx(p_data_selected, file_path)
```


# MRI preprocessing
```{r}
name = colnames(train_functional_connectome)
name[19700:19901]
```
0-199
1+2+3...+199
200*200/2 - (200/2); no diagonal
19900
```{r}
data = train_functional_connectome
participant_ids <- data$participant_id
conn_data <- data[ , -which(names(data) == "participant_id")]


# -------------------------------
# 2. Define a Function to Reconstruct the Full Matrix
# -------------------------------
# This function takes a vector 'v' (of length 19,900) and returns a 200x200 symmetric matrix
reconstruct_matrix <- function(v, n = 200) {
  m <- matrix(0, n, n)         # create an empty matrix
  m[upper.tri(m)] <- v         # fill in the upper triangle (diagonal remains 0)
  m[lower.tri(m)] <- t(m)[lower.tri(m)]  # mirror the upper triangle to the lower triangle
  return(m)
}

# -------------------------------
# 3. Reconstruct and Visualize a Single Subject's Connectivity Matrix
# -------------------------------
# For example, reconstruct the matrix for the first participant
first_vector <- as.numeric(conn_data[100, ])
first_matrix <- reconstruct_matrix(first_vector, n = 200)

# Visualize this matrix as a heatmap
pheatmap(first_matrix,
         main = paste("Connectivity Matrix for Participant", participant_ids[1]),
         cluster_rows = TRUE,
         cluster_cols = TRUE)

```


```{r}

# -------------------------------
# 4. Compute an Average Connectivity Matrix Across All Subjects
# -------------------------------
# Initialize a matrix to accumulate connectivity values
nSubjects <- nrow(conn_data)
nNodes <- 200
avg_matrix <- matrix(0, nNodes, nNodes)

# Loop over subjects and accumulate matrices
for (i in 1:nSubjects) {
  vec <- as.numeric(conn_data[i, ])
  m <- reconstruct_matrix(vec, n = nNodes)
  avg_matrix <- avg_matrix + m
}
avg_matrix <- avg_matrix / nSubjects  # compute the average

# Visualize the average connectivity matrix
pheatmap(avg_matrix,
         main = "Average Connectivity Matrix Across Subjects",
         cluster_rows = TRUE,
         cluster_cols = TRUE)

```

https://pmc.ncbi.nlm.nih.gov/articles/PMC5008686/#S3


https://rdrr.io/github/EBlini/FCnet/f/README.md
```{r}
library("FCnet")
```

```{r}
data("MeanFC") #the object MeanFC is now available

set.seed(1)

N_subs= 50 #number of participants/matrices
subjs_variability= 0.2 #variability (sd) between matrices

#this function creates several matrices by adding gaussian noise
#with sd "variability" to a reference matrix
m_start= simulateMat(mat = MeanFC, 
                     Nmat = N_subs, 
                     mat_variability = subjs_variability)
```

Feature reduction
A-NPE software; time-series; not suitable for us

FCnet: PCA & elestic net

LDA;

spatial;

# Model training

```{r}

# Load the dataset
df <- train_functional_connectome

df_clean <- df[complete.cases(df), ]


# Function to calculate Z-scores for each column
z_score <- function(x) {
  return((x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE))
}

# Apply Z-score function to all numeric columns (exclude patient ID)
z_scores <- as.data.frame(lapply(df_clean[,-1], z_score))  # Compute Z-scores

# Identify rows where any feature has a Z-score above 3 (outlier)
outlier_rows <- apply(z_scores, 1, function(row) any(abs(row) > 3))

# Remove outliers
df_clean <- df_clean[!outlier_rows, ]

# Print how many outliers were removed
cat("Removed", sum(outlier_rows), "outliers.\n")

summary(training_solutions)
```


fit a ML model like this procedure:
PCA select variable; then used the selected variables train RF/XGB/LDA/elastic net; I want 5-fold cv; and comparision between model fitting

training variables:
train_categorical_metadata <- read_excel(file.path(path, "TRAIN_CATEGORICAL_METADATA.xlsx"))
train_functional_connectome <- read_csv(file.path(path2, "TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv"))
train_quantitative_metadata <- read_excel(file.path(path, "TRAIN_QUANTITATIVE_METADATA.xlsx"))

training outcomes:include 2 columns(sex and ADHD)
training_solutions <- read_excel(file.path(path, "TRAINING_SOLUTIONS.xlsx"))


UMAP;

```{r}
# Install LightGBM if not installed
if (!requireNamespace("lightgbm", quietly = TRUE)) {
    install.packages("lightgbm", repos = "https://cran.r-project.org")
}

# Load necessary libraries
library(lightgbm)
library(caret)  # For model training

```


```{r}
# Load required libraries
library(tidyverse)       # Data wrangling
library(readxl)          # Read Excel files
library(caret)           # ML framework & cross-validation
library(glmnet)          # Elastic Net
library(randomForest)    # Random Forest
library(xgboost)         # XGBoost
library(MASS)            # LDA
library(factoextra)      # PCA visualization

# Check structure
summary(train_categorical_metadata)
summary(train_functional_connectome)
summary(train_quantitative_metadata)
glimpse(training_solutions)

```

```{r}
# Ensure all datasets have a common key (e.g., "participant_id")
merged_data <- train_categorical_metadata %>%
  inner_join(train_quantitative_metadata, by = "participant_id") %>%
  inner_join(train_functional_connectome, by = "participant_id") %>%
  inner_join(training_solutions, by = "participant_id")

# Drop participant_id column (not needed for ML)
merged_data <- merged_data %>% dplyr::select(-participant_id)

# Convert categorical variables (sex, ADHD) to factors
merged_data$sex <- as.factor(merged_data$sex)
merged_data$ADHD <- as.factor(merged_data$ADHD)

```

checked no missing; outlier might matter

```{r}
# Identify functional connectome columns based on pattern
connectome_columns <- grep("^[0-9]+throw_[0-9]+thcolumn$", names(merged_data), value = TRUE)

# Extract functional connectome data
connectome_data <- merged_data %>% dplyr::select(all_of(connectome_columns))

# Extract non-connectome metadata & outcomes
metadata <- merged_data %>% dplyr::select(-all_of(connectome_columns))
colnames(metadata)
```

```{r}
# Perform PCA on functional connectome data
pca_result <- prcomp(connectome_data, center = TRUE, scale. = FALSE)  # No scaling

# Determine number of components explaining 95% variance
explained_variance <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
num_pcs <- which(explained_variance >= 0.95)[1]

# Select PCA components for training
pca_data <- as.data.frame(pca_result$x[, 1:num_pcs])
pca_data$sex <- metadata$Sex_F
pca_data$ADHD <- metadata$ADHD_Outcome

# Check PCA variance plot
fviz_eig(pca_result)

```

```{r}
# Define cross-validation method (5-fold CV)
control <- trainControl(method = "cv", number = 5)

# Define features (X) and labels (Y)
X <- pca_data %>% dplyr::select(-sex, -ADHD)
Y_sex <- pca_data$sex
Y_ADHD <- pca_data$ADHD


Y_sex <- factor(Y_sex)
Y_ADHD <- factor(Y_ADHD)
```

```{r}
set.seed(123)
rf_model_sex <- train(X, Y_sex, method = "rf", trControl = control, tuneLength = 5)
rf_model_ADHD <- train(X, Y_ADHD, method = "rf", trControl = control, tuneLength = 5)

saveRDS(rf_model_sex, "rf_model_sex.rds")
saveRDS(rf_model_ADHD, "rf_model_ADHD.rds")

```

```{r}
lgb_train_sex <- lgb.Dataset(as.matrix(X), label = as.numeric(Y_sex) - 1)
lgb_train_ADHD <- lgb.Dataset(as.matrix(X), label = as.numeric(Y_ADHD) - 1)

# **(B) LightGBM Tuning**
lgb_params <- list(
    objective = "binary",
    metric = "auc",
    boosting = "gbdt",
    num_leaves = 31,
    learning_rate = 0.05,
    feature_fraction = 0.8,
    lambda_l1 = 0.1,   # L1 regularization (Lasso)
    lambda_l2 = 0.1    # L2 regularization (Ridge)
)

# Train LightGBM for Sex classification
set.seed(123)
lightgbm_model_sex <- lgb.train(params = lgb_params, data = lgb_train_sex, nrounds = 100)

# Train LightGBM for ADHD classification
set.seed(123)
lightgbm_model_ADHD <- lgb.train(params = lgb_params, data = lgb_train_ADHD, nrounds = 100)


saveRDS(lightgbm_model_sex, "lightgbm_model_sex.rds")
saveRDS(lightgbm_model_ADHD, "lightgbm_model_ADHD.rds")

```

```{r}
# Define 5-fold cross-validation

# Define XGBoost parameters
xgb_grid <- expand.grid(
    nrounds = c(100, 200),          # Number of boosting rounds
    max_depth = c(3, 6, 9),         # Depth of trees
    eta = c(0.01, 0.05, 0.1),       # Learning rate
    gamma = c(0, 1),                # Minimum loss reduction
    colsample_bytree = c(0.7, 1),   # Subsampling for columns
    min_child_weight = c(1, 3),     # Minimum sum of instance weight
    subsample = c(0.7, 1)           # Fraction of samples used per tree
)

# Train XGBoost model for Sex classification
set.seed(123)
xgb_model_sex <- train(
    X, Y_sex,
    method = "xgbTree",
    trControl = control,
    tuneGrid = xgb_grid
)

# Train XGBoost model for ADHD classification
set.seed(123)
xgb_model_ADHD <- train(
    X, Y_ADHD,
    method = "xgbTree",
    trControl = control,
    tuneGrid = xgb_grid
)


saveRDS(xgb_model_sex, "xgb_model_sex.rds")
saveRDS(xgb_model_ADHD, "xgb_model_ADHD.rds")

```


```{r}
set.seed(123)
# Convert outcomes to factors (binary classification)


lda_model_sex <- train(X, Y_sex, method = "lda", trControl = control)
lda_model_ADHD <- train(X, Y_ADHD, method = "lda", trControl = control)

saveRDS(lda_model_sex, "lda_model_sex.rds")
saveRDS(lda_model_ADHD, "lda_model_ADHD.rds")

```

```{r}
# Train LDA with Regularization
set.seed(123)
lda_model_ADHD_reg <- train(X, Y_ADHD, method = "sda", trControl = control)  # "sda" = Shrinkage Discriminant Analysis
lda_model_sex_reg <- train(X, Y_sex, method = "sda", trControl = control)


saveRDS(lda_model_sex_reg, "lda_model_sex_reg.rds")
saveRDS(lda_model_ADHD_reg, "lda_model_ADHD_reg.rds")
```


```{r}
# **(D) Elastic Net Tuning**
elastic_grid <- expand.grid(alpha = seq(0, 1, by = 0.1), lambda = 10^seq(-3, 3, length = 10))
set.seed(123)
elastic_net_model_sex <- train(X, Y_sex, method = "glmnet", trControl = control, tuneGrid = elastic_grid)
elastic_net_model_ADHD <- train(X, Y_ADHD, method = "glmnet", trControl = control, tuneGrid = elastic_grid)

saveRDS(elastic_net_model_sex, "elastic_net_model_sex.rds")
saveRDS(elastic_net_model_ADHD, "elastic_net_model_ADHD.rds")

```


```{r}
# Compare model performance
results_sex <- resamples(list(#RF = rf_model_sex, 
                              LightGBM = lightgbm_model_sex, 
                              LDA = lda_model_sex, 
                              LDA_reg = lda_model_sex_reg,
                              ElasticNet = elastic_net_model_sex))
results_ADHD <- resamples(list(#RF = rf_model_ADHD, 
                               LightGBM = lightgbm_model_ADHD, 
                               LDA = lda_model_ADHD, 
                               LDA_reg = lda_model_ADHD_reg,
                               ElasticNet = elastic_net_model_ADHD))

# Print model performance summary
summary(results_sex)
summary(results_ADHD)



```

```{r}
rf_model_ADHD
lightgbm_model_ADHD
lda_model_ADHD
elastic_net_model_ADHD

```

```{r}
# Boxplot for model performance comparison
plot(results_sex)
bwplot(results_ADHD)
bwplot(results_sex)  # Change to "ROC" if available


# Save trained models
saveRDS(rf_model_sex, "rf_model_sex.rds")
saveRDS(lightgbm_model_sex, "lightgbm_model_sex.rds")
saveRDS(lda_model_sex, "lda_model_sex.rds")
saveRDS(elastic_net_model_sex, "elastic_net_model_sex.rds")

saveRDS(rf_model_ADHD, "rf_model_ADHD.rds")
saveRDS(lightgbm_model_ADHD, "lightgbm_model_ADHD.rds")
saveRDS(lda_model_ADHD, "lda_model_ADHD.rds")
saveRDS(elastic_net_model_ADHD, "elastic_net_model_ADHD.rds")
```

